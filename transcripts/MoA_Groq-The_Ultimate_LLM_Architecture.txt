{"speakers": [], "chunks": [{"timestamp": [0.0, 3.62], "text": " So Grok just dropped Mixture of Agents natively."}, {"timestamp": [3.78, 4.96], "text": " For those of you who don't remember,"}, {"timestamp": [5.12, 6.7], "text": " Mixture of Agents allows you to take"}, {"timestamp": [6.7, 8.58], "text": " quote-unquote less capable models"}, {"timestamp": [8.58, 13.0], "text": " and make them incredibly capable, nearly GPT-4.0 level."}, {"timestamp": [13.24, 16.18], "text": " And I made an entire video about exactly how it works."}, {"timestamp": [16.32, 17.88], "text": " I'll link that in the description below."}, {"timestamp": [18.04, 20.34], "text": " But today, I'm gonna show you how to set it all up."}, {"timestamp": [20.4, 21.6], "text": " And it's actually really easy,"}, {"timestamp": [21.72, 23.78], "text": " so this video's gonna be on the shorter side."}, {"timestamp": [23.92, 29.76], "text": " The only thing you're gonna need is VS Code and a Grok API key. First open up VS Code. Then"}, {"timestamp": [29.76, 34.32], "text": " you're going to cd into whatever directory you like to store your project. So for me when I'm"}, {"timestamp": [34.32, 38.32], "text": " playing around with it I like to go to the desktop. Now here's the project repository"}, {"timestamp": [38.32, 43.52], "text": " and it was created by Saomi from Grok. So thank you so much to him for putting this together."}, {"timestamp": [43.52, 45.36], "text": " It is so much better than my hacky"}, {"timestamp": [45.36, 50.88], "text": " version that I put together and it comes with an interface so it is especially good. So again,"}, {"timestamp": [50.88, 55.68], "text": " this is how mixture of agents works. Here's the prompt. It goes through multiple agents work"}, {"timestamp": [55.68, 61.44], "text": " together over multiple layers to come up with the best possible output. Now when I first read this"}, {"timestamp": [61.44, 67.64], "text": " paper, what came to mind immediately is it's probably pretty slow and they actually pointed that out in the paper."}, {"timestamp": [67.64, 72.04], "text": " But of course, immediately my mind went to let's power this with Grok because then you"}, {"timestamp": [72.04, 74.66], "text": " have the massive speed advantage."}, {"timestamp": [74.66, 77.62], "text": " So I will drop the link to this in the description below."}, {"timestamp": [77.62, 82.46], "text": " So back to VS code, you're going to type git clone and then the GitHub URL for this project"}, {"timestamp": [82.46, 84.8], "text": " and switching back, you can find it right here."}, {"timestamp": [84.8, 86.86], "text": " So if you click this little green code button,"}, {"timestamp": [86.86, 88.78], "text": " there's a little copy button right there."}, {"timestamp": [88.78, 90.64], "text": " Then you're gonna hit enter."}, {"timestamp": [90.64, 94.8], "text": " Once that's done, let's cd into grocmoa."}, {"timestamp": [94.8, 96.92], "text": " Now we're in that project directory."}, {"timestamp": [96.92, 99.3], "text": " Next, you're gonna wanna click this Explorer button,"}, {"timestamp": [99.3, 101.22], "text": " open folder, go to your desktop,"}, {"timestamp": [101.22, 103.78], "text": " and then click that new project that you just downloaded"}, {"timestamp": [103.78, 104.74], "text": " and click open."}, {"timestamp": [104.74, 108.8], "text": " Okay, now we have it open. Let's open up the terminal down here again. The next thing we're going to"}, {"timestamp": [108.8, 115.94], "text": " do is create a new conda environment. So conda create dash n grok dash moa python equals 3.11."}, {"timestamp": [116.12, 120.34], "text": " Then hit enter, hit enter again to confirm. And then when it's done, you're going to grab this"}, {"timestamp": [120.34, 130.26], "text": " command right here to activate it, copy, paste it into terminal, and then hit enter. And you can see it's active because it says so right there. Next, we need to install all of the"}, {"timestamp": [130.26, 136.84], "text": " dependencies. So we're going to type pip install dash r requirements dot txt, then hit enter. And"}, {"timestamp": [136.84, 141.84], "text": " I didn't run into a single problem while getting this spun up. So hopefully you don't either. And"}, {"timestamp": [141.84, 149.92], "text": " it also looks like it comes with a Docker file if you wanted to use that. Next, you're going to right click in this project directory and hit new file and type"}, {"timestamp": [149.92, 154.64], "text": " dot ENV because we need to set our environment variables. From there, it's just going to be one"}, {"timestamp": [154.64, 160.48], "text": " line grok underscore API underscore key equals and then you place your grok API key right here. Now,"}, {"timestamp": [160.48, 165.18], "text": " if you don't already have a grok account, go ahead and sign up, go to console.grok.com"}, {"timestamp": [165.18, 170.98], "text": " slash keys, create key. We're going to type M-O-A-Y-T. So I know it's for YouTube. Hit"}, {"timestamp": [170.98, 175.58], "text": " enter. I am going to revoke this key before publishing the video. Let's copy, switch back"}, {"timestamp": [175.58, 180.08], "text": " to VS code, paste it in and hit save. And we're pretty much done. So last to get it"}, {"timestamp": [180.08, 184.98], "text": " up and running, you're going to type streamlit run app.py and then hit enter. All right."}, {"timestamp": [184.98, 185.36], "text": " Then it opens up local hosts. So this is running locally. It'll take a second to spin up and running, you're going to type streamlit run app.py and then hit enter. All right, then it"}, {"timestamp": [185.36, 190.64], "text": " opens up local host. So this is running locally, it'll take a second to spin up. And there we go,"}, {"timestamp": [190.82, 195.32], "text": " this is the interface. So let me give you a demo of it really quickly. So on the left side, this is"}, {"timestamp": [195.32, 200.06], "text": " where we can have all of our settings. And we can select our main model. And we probably want the"}, {"timestamp": [200.06, 205.0], "text": " most capable model. So right now llama 370 B sounds good. We can actually subtract"}, {"timestamp": [205.0, 209.42], "text": " and add number of layers so you can experiment with the number of layers that works best"}, {"timestamp": [209.42, 214.1], "text": " for your use case. Now, according to the paper, three layers seems to be optimal and that"}, {"timestamp": [214.1, 216.9], "text": " is the default for this project. So we're going to leave it at three. But of course,"}, {"timestamp": [216.9, 220.42], "text": " if you want to play around with it and experiment, see what works for you, please do. Then we"}, {"timestamp": [220.42, 224.12], "text": " have the main model temperature. I am going to leave it where it is. But again, something"}, {"timestamp": [224.12, 225.54], "text": " else you can experiment with."}, {"timestamp": [225.66, 227.76], "text": " And here is where we can actually customize"}, {"timestamp": [227.76, 229.3], "text": " the agents for each layer."}, {"timestamp": [229.4, 231.36], "text": " So we have layer agent one, layer agent two,"}, {"timestamp": [231.4, 232.28], "text": " and layer agent three."}, {"timestamp": [232.4, 235.44], "text": " We're gonna be using Lama 3 8B, Gemma 7B,"}, {"timestamp": [235.7, 237.32], "text": " and then Lama 3 8B again."}, {"timestamp": [237.32, 239.08], "text": " And of course, Grok has other models"}, {"timestamp": [239.08, 240.2], "text": " that you can play around with."}, {"timestamp": [240.34, 243.24], "text": " And you would just change the model name right there."}, {"timestamp": [243.28, 244.8], "text": " And you can also change the temperature"}, {"timestamp": [244.8, 246.82], "text": " and other settings as you see fit."}, {"timestamp": [246.94, 250.22], "text": " So with that set, let's give it a try."}, {"timestamp": [250.5, 252.66], "text": " Write 10 sentences that end with the word Apple."}, {"timestamp": [252.84, 258.14], "text": " Now it is going through multiple layers with multiple agents, but still look how fast that is."}, {"timestamp": [258.26, 263.18], "text": " And the cool thing is you can actually dig into each layer and each agent to see what they've done."}, {"timestamp": [263.34, 264.88], "text": " So here's layer one, agent one."}, {"timestamp": [264.98, 266.34], "text": " Here's the output."}, {"timestamp": [266.34, 269.52], "text": " And it looks like layer one, agent one"}, {"timestamp": [269.52, 271.4], "text": " actually got it right right out of the box."}, {"timestamp": [271.4, 274.14], "text": " Layer one, agent two, thought and response."}, {"timestamp": [274.14, 276.62], "text": " So that is a really poor response"}, {"timestamp": [276.62, 278.24], "text": " and that just happens to be Gemma."}, {"timestamp": [278.24, 280.94], "text": " And then agent three, looks like it almost got it right"}, {"timestamp": [280.94, 283.14], "text": " but here's one sentence that it did not."}, {"timestamp": [283.14, 284.62], "text": " So you can see that was layer one,"}, {"timestamp": [284.62, 289.0], "text": " now we have layer two of the three agents and layer three. And then at the end of it,"}, {"timestamp": [289.0, 294.5], "text": " we have llama three 70 B putting it all together and it got the right answer and it was super"}, {"timestamp": [294.5, 299.24], "text": " fast too. So really cool project out of grok. Thank you for making mixture of agents native"}, {"timestamp": [299.24, 303.44], "text": " to grok. Now I actually hope they build this into the main interface as an option. And"}, {"timestamp": [303.44, 307.32], "text": " I hope inference companies actually start building a lot of these types of things."}, {"timestamp": [307.44, 310.12], "text": " Mixture of agents, route LLM, chain of thought."}, {"timestamp": [310.36, 314.04], "text": " All of these things should be built into the inference interface."}, {"timestamp": [314.38, 315.86], "text": " So a couple last things."}, {"timestamp": [315.96, 318.0], "text": " Up in the top right, we have a deploy button."}, {"timestamp": [318.08, 321.22], "text": " If you wanted to deploy this with Streamlit, it makes it very easy."}, {"timestamp": [321.44, 325.14], "text": " We also, under the three dots, have rerun, which we can do just like that."}, {"timestamp": [325.26, 331.72], "text": " We also have different settings. So run on save, wide mode, and the app theme. You can print,"}, {"timestamp": [331.88, 336.16], "text": " you can record a screencast, which I wasn't expecting to be in here. And you can also clear"}, {"timestamp": [336.16, 341.92], "text": " the cache. So a lot of cool things. I have a feeling this project is going to evolve quite a"}, {"timestamp": [341.92, 346.1], "text": " bit. So I'm really excited to see it. So check it out. I'll drop all the links in the description below."}, {"timestamp": [346.3, 347.16], "text": " If you liked this video,"}, {"timestamp": [347.26, 348.54], "text": " please consider giving a like and subscribe"}, {"timestamp": [348.54, 349.94], "text": " and I'll see you in the next one."}], "text": " So Grok just dropped Mixture of Agents natively. For those of you who don't remember, Mixture of Agents allows you to take quote-unquote less capable models and make them incredibly capable, nearly GPT-4.0 level. And I made an entire video about exactly how it works. I'll link that in the description below. But today, I'm gonna show you how to set it all up. And it's actually really easy, so this video's gonna be on the shorter side. The only thing you're gonna need is VS Code and a Grok API key. First open up VS Code. Then you're going to cd into whatever directory you like to store your project. So for me when I'm playing around with it I like to go to the desktop. Now here's the project repository and it was created by Saomi from Grok. So thank you so much to him for putting this together. It is so much better than my hacky version that I put together and it comes with an interface so it is especially good. So again, this is how mixture of agents works. Here's the prompt. It goes through multiple agents work together over multiple layers to come up with the best possible output. Now when I first read this paper, what came to mind immediately is it's probably pretty slow and they actually pointed that out in the paper. But of course, immediately my mind went to let's power this with Grok because then you have the massive speed advantage. So I will drop the link to this in the description below. So back to VS code, you're going to type git clone and then the GitHub URL for this project and switching back, you can find it right here. So if you click this little green code button, there's a little copy button right there. Then you're gonna hit enter. Once that's done, let's cd into grocmoa. Now we're in that project directory. Next, you're gonna wanna click this Explorer button, open folder, go to your desktop, and then click that new project that you just downloaded and click open. Okay, now we have it open. Let's open up the terminal down here again. The next thing we're going to do is create a new conda environment. So conda create dash n grok dash moa python equals 3.11. Then hit enter, hit enter again to confirm. And then when it's done, you're going to grab this command right here to activate it, copy, paste it into terminal, and then hit enter. And you can see it's active because it says so right there. Next, we need to install all of the dependencies. So we're going to type pip install dash r requirements dot txt, then hit enter. And I didn't run into a single problem while getting this spun up. So hopefully you don't either. And it also looks like it comes with a Docker file if you wanted to use that. Next, you're going to right click in this project directory and hit new file and type dot ENV because we need to set our environment variables. From there, it's just going to be one line grok underscore API underscore key equals and then you place your grok API key right here. Now, if you don't already have a grok account, go ahead and sign up, go to console.grok.com slash keys, create key. We're going to type M-O-A-Y-T. So I know it's for YouTube. Hit enter. I am going to revoke this key before publishing the video. Let's copy, switch back to VS code, paste it in and hit save. And we're pretty much done. So last to get it up and running, you're going to type streamlit run app.py and then hit enter. All right. Then it opens up local hosts. So this is running locally. It'll take a second to spin up and running, you're going to type streamlit run app.py and then hit enter. All right, then it opens up local host. So this is running locally, it'll take a second to spin up. And there we go, this is the interface. So let me give you a demo of it really quickly. So on the left side, this is where we can have all of our settings. And we can select our main model. And we probably want the most capable model. So right now llama 370 B sounds good. We can actually subtract and add number of layers so you can experiment with the number of layers that works best for your use case. Now, according to the paper, three layers seems to be optimal and that is the default for this project. So we're going to leave it at three. But of course, if you want to play around with it and experiment, see what works for you, please do. Then we have the main model temperature. I am going to leave it where it is. But again, something else you can experiment with. And here is where we can actually customize the agents for each layer. So we have layer agent one, layer agent two, and layer agent three. We're gonna be using Lama 3 8B, Gemma 7B, and then Lama 3 8B again. And of course, Grok has other models that you can play around with. And you would just change the model name right there. And you can also change the temperature and other settings as you see fit. So with that set, let's give it a try. Write 10 sentences that end with the word Apple. Now it is going through multiple layers with multiple agents, but still look how fast that is. And the cool thing is you can actually dig into each layer and each agent to see what they've done. So here's layer one, agent one. Here's the output. And it looks like layer one, agent one actually got it right right out of the box. Layer one, agent two, thought and response. So that is a really poor response and that just happens to be Gemma. And then agent three, looks like it almost got it right but here's one sentence that it did not. So you can see that was layer one, now we have layer two of the three agents and layer three. And then at the end of it, we have llama three 70 B putting it all together and it got the right answer and it was super fast too. So really cool project out of grok. Thank you for making mixture of agents native to grok. Now I actually hope they build this into the main interface as an option. And I hope inference companies actually start building a lot of these types of things. Mixture of agents, route LLM, chain of thought. All of these things should be built into the inference interface. So a couple last things. Up in the top right, we have a deploy button. If you wanted to deploy this with Streamlit, it makes it very easy. We also, under the three dots, have rerun, which we can do just like that. We also have different settings. So run on save, wide mode, and the app theme. You can print, you can record a screencast, which I wasn't expecting to be in here. And you can also clear the cache. So a lot of cool things. I have a feeling this project is going to evolve quite a bit. So I'm really excited to see it. So check it out. I'll drop all the links in the description below. If you liked this video, please consider giving a like and subscribe and I'll see you in the next one."}